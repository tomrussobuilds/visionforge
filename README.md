# BloodMNIST Classification with Adapted ResNet-18

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![TorchVision](https://img.shields.io/badge/TorchVision-0.15%2B-red)



**97.11% Test Accuracy â€¢ 96.78 Macro F1 â€¢ Single pretrained ResNet-18 â€¢ 28Ã—28 images**

This repository provides a reproducible training pipeline for the BloodMNIST (from MedMNIST v2) using an adapted pretrained ResNet-18, demonstrating solid performance with a straightforward setup.

### Confusion Matrix
<img src="figures/confusion_matrix_resnet18.png" width="400">

### Training Curves
<img src="figures/training_curves.png" width="400">

### Sample Predictions
<img src="figures/sample_predictions.png" width="400">

### Final Results (60 epochs, seed 42)
| Metric                  | Value     |
|-------------------------|-----------|
| Best Validation Accuracy| **97.55%** |
| Test Accuracy (with TTA)| **97.11%** |
| Test Macro F1 (with TTA)| **0.9678** |

â†’ Confusion matrix, training curves, sample predictions and Excel report are automatically saved.

---

### Why this repo exists

I wanted to see how far a **single pretrained ResNet-18** could go on the tiny 28Ã—28 BloodMNIST dataset with proper adaptation and modern training practices â€” no Ensembles, no ViTs, no custom backbones.

Spoiler: a carefully adapted ResNet-18 performs surprisingly well, even on 28Ã—28 medical images.

---

### Key Features & Design Choices

- **ResNet-18 adapted for 28Ã—28**:  
  â€“ Replaced 7Ã—7 conv with 3Ã—3 (preserves spatial info)  
  â€“ Removed initial MaxPool â†’ full 28Ã—28 feature maps until the end  
  â€“ ImageNet pretrained weights transferred via bicubic upsampling of the first conv

- Strong but reasonable data augmentation + very light **MixUp** (Î± = 0.001 â€“ kept silent on purpose, higher values hurt here)

- Cosine annealing for first ~33 epochs â†’ ReduceLROnPlateau afterwards

- Test-Time Augmentation (7 deterministic transforms, averaged)

- Automatic dataset download with MD5 validation and atomic write

- Full reproducibility (fixed seeds, deterministic CuDNN)

- Exhaustive logging, Excel report, confusion matrix, training curves, sample predictions

- A ridiculous amount of defensive utilities born from real pain at 5 AM debugging sessions (see below)

---

### The Small Utilities That Save Large Headaches

A few tiny helpers included in this repo were added after very real 5AM debugging incidents:

- **`get_base_dir()`** â€” ensures outputs never end up in unexpected system locations  
- **`kill_duplicate_processes()`** â€” stops accidental multi-launches that hog all RAM  
- **`ensure_mnist_npz()`** â€” safe dataset download with retries, MD5 check, and atomic write  
- Graceful process cleanup, checksum utilities, debug-safe file creation, etc.

They may look overkill, but they make the whole training pipeline safe to run unattended.

---

### Project Structure

```bash
bloodmnist/
â”‚
â”œâ”€â”€ train_bloodmnist.py       # Main training script
â”‚
â”œâ”€â”€ dataset/                  # BloodMNIST dataset
â”œâ”€â”€ logs/                     # Logs to file
â”œâ”€â”€ figures/                  # Auto-generated plots
â”œâ”€â”€ reports/                  # Excel report + logs
â””â”€â”€ models/                   # Saved checkpoints
```

### Requirements

```bash
pip install -r requirements.txt
```

Install dependencies easily with pip, or check the full list here:

[ðŸ“¦ See Full Requirements](requirements.txt)


### Usage

```bash
git clone https://github.com/tomrussobuilds/bloodmnist.git
cd bloodmnist
python train_bloodmnist.py
```

You can also check the training script directly: 

[ðŸ“„ train_bloodmnist.py](train_bloodmnist.py)

Thatâ€™s it.
The script will:

- Download BloodMNIST if missing
- Train for max 60 epochs with early stopping (patience=15)
- Save the best model â†’ models/resnet18_bloodmnist_best.pth
- Generate figures, confusion matrix, Excel report â†’ figures/ and reports/

### Command Line Arguments (argparse)
You can now fully configure training from the command line.

| Arg | Type | Default | Description |
| :--- | :--- | :--- | :--- |
| --epochs | int | 60 | Maximum number of training epochs. |
| --batch_size | int | 128 | Batch size for data loaders. |
| --lr | float | 0.008 | Initial learning rate for the SGD optimizer. |
| --seed | int | 42 | Random seed for reproducibility (influences PyTorch, NumPy, Python). |
| --mixup_alpha | float | 0.001 | $\alpha$ parameter for MixUp regularization. Set to 0 to disable MixUp. |
| --patience | int | 15 | Early stopping patience (epochs without validation improvement). |
| --no_tta | flag | (disabled) | Flag to disable Test-Time Augmentation (TTA) during final evaluation. |

Examples:

Run without TTA

```bash
python train_bloodmnist.py --no_tta
```
Train for 100 epochs

```bash
python train_bloodmnist.py --epochs 100
```
Lower LR for finer tuning

```bash
python train_bloodmnist.py --lr 0.001
```
Disable MixUp

```bash
python train_bloodmnist.py --mixup_alpha 0
```
Custom batch size & seed

```bash
python train_bloodmnist.py --barch_size 64 --seed 123
```

### Reproducibility

Everything is deterministic (seed 42). Run the script twice â†’ same validation curve, same final accuracy.

### Citation

If you use this repository in academic work or derivative projects:

```bibtex
@misc{bloodmnist_resnet18,
  title  = {BloodMNIST Classification with Adapted ResNet-18},
  author = {Tommaso Russo},
  year   = {2025},
  url    = {https://github.com/tomrussobuilds/bloodmnist}
}
```

### Conclusion

This project shows how a classic, lightweight architecture like ResNet-18 can perform extremely well on a compact medical-image dataset when paired with a careful training setup.  

The goal is not to chase leaderboard scores, but to provide a **clean, stable, reproducible** pipeline that others can reuse or extend with minimal friction.

If you find this project useful, feedback and suggestions are always welcome.
