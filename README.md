# BloodMNIST Classification with Adapted ResNet-18

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![TorchVision](https://img.shields.io/badge/TorchVision-0.15%2B-red)



**97.19% Test Accuracy â€¢ 0.9693 Macro F1 â€¢ Single pretrained ResNet-18 â€¢ 28Ã—28 images**

This repository provides a highly reproducible training pipeline for the BloodMNIST (from MedMNIST v2) using an adapted pretrained ResNet-18 architecture. The goal is to demonstrate solid performance using a minimal configuration that adheres to modern PyTorch best practices.

The results reflect the latest successful training run (early stopping at epoch 44).

### Confusion Matrix
<img src="figures/confusion_matrix_resnet18.png" width="400">

### Training Curves
<img src="figures/training_curves.png" width="400">

### Sample Predictions
<img src="figures/sample_predictions.png" width="400">

### Final Results (60 epochs, seed 42)
| Metric                  | Value     |
|-------------------------|-----------|
| Best Validation Accuracy| **96.96%** |
| Test Accuracy (with TTA)| **97.19%** |
| Test Macro F1 (with TTA)| **0.9693** |

â†’ Confusion matrix, training curves, sample predictions and Excel report are automatically saved.

---

### Why this repo exists

I wanted to see how far a **single pretrained ResNet-18** could go on the tiny 28Ã—28 BloodMNIST dataset with proper adaptation and modern training practices â€” no Ensembles, no ViTs, no custom backbones.

Spoiler: a carefully adapted ResNet-18 performs surprisingly well, even on 28Ã—28 medical images.

### Key Features & Design Choices (Post-Refactoring)

- **Modularity and Structure**: Code has been separated into clean, modular components (`scripts/`) to enforce single responsibility principle and decouple execution (`main.py`) from business logic.
- **Robust Pathing**: Implemented dynamic `PROJECT_ROOT` detection in `utils.py` to ensure all outputs (models, logs, figures) are correctly saved relative to the project root, regardless of where the script is executed (local or Docker container).
- **Accuracy vs. Reproducibility Balance**: **The pipeline prioritizes fully deterministic reproducibility.** While running in "Fast Mode" (`num_workers > 0`) is faster, the "Strict Reproducibility" mode (`num_workers=0`) guarantees bit-per-bit identical results at the expense of a longer training time. This trade-off is managed automatically via `DOCKER_REPRODUCIBILITY_MODE` environment variable.

---


### ResNet-18 adapted for 28Ã—28
- Initial 7 x7 convolution replaced with 3 x 3.
- Initial `MaxPool` removed to preserve 28 x 28 feature maps.
- ImageNet pretrained weights transferred via bicubic upsampling of the first convolutional layer.
- Reproduciblity & Robustness: â€“ Full Reproducibility guaranteed (fixed seeds for PyTorch, NumPy, Python).
- `worker_init_fn` implemented to ensure determinism even when using multiple data loading workers (`num_workers > 0`).
- Defensive Utilities: Robust dataset download with MD5 validation and atomic write ensures pipeline reliablity.

---

### Defensive Utilities

A few tiny helpers included in `utils.py` were added after real debugging incidents to ensure robust, unattended execution:

* **Dynamic `num_workers`:** The `Config` class automatically adjusts `num_workers` (0 vs 4) based on the `DOCKER_REPRODUCIBILITY_MODE` environment variable, balancing speed and determinism.
* **Process Management (`kill_duplicate_processes()`):** Stops accidental multi-launches that consume excessive CPU/RAM.
* **Safe Data I/O (`ensure_mnist_npz()`):** Robust dataset download with retries, MD5 check, and atomic write ensures pipeline reliability.
* **Robust Pathing:** The `get_base_dir()` utility ensures all outputs (models, logs, figures) are saved correctly relative to the project root, regardless of the execution environment (host or Docker).

---

### Project Structure

```bash
bloodmnist/
â”‚
â”œâ”€â”€ main.py                   # Main execution entry point
â”‚
â”œâ”€â”€ scripts/                  # Core modules (data loading, models, trainer, utils)
â”‚   â”œâ”€â”€ data_handler.py       # Handles data download, Dataset, and DataLoader setup
â”‚   â”œâ”€â”€ models.py             # Defines the adapted ResNet-18 model
â”‚   â”œâ”€â”€ trainer.py            # Contains the training logic (loop, MixUp, Early Stopping)
â”‚   â”œâ”€â”€ utils.py              # Configuration (Config), Logger, and general utilities
â”‚   â””â”€â”€ evaluation.py         # Reporting and final evaluation logic
â”‚
â”œâ”€â”€ dataset/                  # BloodMNIST dataset files
â”œâ”€â”€ logs/                     # File logs
â”œâ”€â”€ figures/                  # Auto-generated plots
â”œâ”€â”€ reports/                  # Excel report + final logs
â””â”€â”€ models/                   # Saved model checkpoints
```

### 1. Requirements

Install dependencies easily with pip:

```bash
pip install -r requirements.txt
```

Install dependencies easily with pip, or check the full list here:

[ðŸ“¦ See Full Requirements](requirements.txt)


### 2. Usage (Local Execution - Recommended for Speed)

Run the script from the project root. It will default to the fast mode (`num_workers=4`).

```bash
git clone https://github.com/tomrussobuilds/bloodmnist.git
cd bloodmnist
python main.py
```
Note: The entry point script is now `main.py`.

The script will automatically:

- Download BloodMNIST if missing
- Train for max 60 epochs with early stopping (`patience=15`)
- Save the best model â†’ `models/resnet18_bloodmnist_best.pth`
- Generate figures, confusion matrix, Excel report â†’ `figures/` and `reports/`

### 3. Docker Execution (Recommended for Portability & Reproducibility)

The pipeline is containerized using the included `Dockerfile`.

| Mode | Command | `num_workers` | Guarantees |
| :--- | :--- | :---: | :--- |
| **Fast Mode (Default)** | `docker run bloodmnist_image` | 4 | Best performance on CPU, **Not fully deterministic** |
| **Strict Reproducibility** | `-e DOCKER_REPRODUCIBILITY_MODE=TRUE` | 0 | **100% Deterministic** (bit-per-bit), but slower |

Run in Strict Reproducibility Mode (Recommended for testing):

```bash
docker run -it --rm -e DOCKER_REPRODUCIBILITY_MODE=TRUE bloodmnist_image
```

### Command Line Arguments (argparse)
You can fully configure training from the command line (via `main.py`).

| Arg | Type | Default | Description |
| :--- | :--- | :--- | :--- |
| --epochs | int | 60 | Maximum number of training epochs. |
| --batch_size | int | 128 | Batch size for data loaders. |
| --lr | float | 0.008 | Initial learning rate for the SGD optimizer. |
| --seed | int | 42 | Random seed for reproducibility (influences PyTorch, NumPy, Python). |
| --mixup_alpha | float | 0.002 | $\alpha$ parameter for MixUp regularization. Set to 0 to disable MixUp. |
| --patience | int | 15 | Early stopping patience (epochs without validation improvement). |
| --momentum | float | 0.9 | Momentum factor for the SGD optimizer. |
| --weight_decay | float | 5e-4 | Weight decay (L2 penalty) for the optimizer |
| --no_tta | flag | (disabled) | Flag to disable Test-Time Augmentation (TTA) during final evaluation. |

Examples:

Run without TTA

```bash
python main.py --no_tta
```
Train for 100 epochs

```bash
python main.py --epochs 100
```
Lower LR for finer tuning

```bash
python main.py --lr 0.001
```
Disable MixUp

```bash
python main.py --mixup_alpha 0
```
Custom batch size & seed

```bash
python main.py --batch_size 64 --seed 123
```

### Reproducibility

The entire pipeline is deterministic (seed 42). Run the script twice will yield the same validation curve and the same final accuracy.

### Citation

If you use this repository in academic work or derivative projects:

```bibtex
@misc{bloodmnist_resnet18,
  title  = {BloodMNIST Classification with Adapted ResNet-18},
  author = {Tommaso Russo},
  year   = {2025},
  url    = {https://github.com/tomrussobuilds/bloodmnist}
}
```

### Conclusion

This project demonstrates how a classic, lightweight architecture like ResNet-18 can perform extremely well on a compact medical-image dataset when paired with a careful, modern training setup.

The goal is to provide a clean, stable, reproducible pipeline that others can reuse or extend with minimal friction.

If you find this project useful, feedback and suggestions are always welcome!
