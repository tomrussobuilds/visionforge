# ğŸ©º MedMNIST Classification with Adapted ResNet-18

![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![TorchVision](https://img.shields.io/badge/TorchVision-0.15%2B-red)



**97.78% Test Accuracy â€¢ 0.9752 Macro F1 â€¢ Single pretrained ResNet-18 â€¢ 28Ã—28 images**

## ğŸ“Œ Table of Contents
* [ğŸš€ Key Features](#-key-features)
* [ğŸ— Architecture Details](#-architecture-details)
* [ğŸ”¬ Training Regularization](#-training-regularization)
* [ğŸ“ Project Structure](#-project-structure)
* [âš™ï¸ Requirements & Installation](#ï¸-requirements--installation)
* [ğŸ’» Usage (Local )](#-usage-local)
* [ğŸ³ Docker Execution](#-docker-execution)
* [âœ… Environment Verification (Smoke Test)](#-environment-verification-smoke-test)
* [ğŸ“Š Command Line Arguments](#-command-line-arguments)
* [ğŸ—º Research Goals](#-research-goals)

This repository provides a highly reproducible, robust training framework for the **MedMNIST v2** suite (supporting BloodMNIST, DermaMNIST, etc.) using an adapted pretrained ResNet-18 architecture. The goal is to demonstrate solid performance using a minimal configuration that adheres to modern PyTorch best practices.


### Final Results (60 epochs, seed 42)

| Metric                  | Value     |
|-------------------------|-----------|
| Best Validation Accuracy| **96.96%** |
| Test Accuracy (with TTA)| **97.78%** |
| Test Macro F1 (with TTA)| **0.9752** |

### Confusion Matrix
<img src="docs/media/confusion_matrix.png" width="400">

### Training Curves
<img src="docs/media/training_curves.png" width="400">

â†’ Confusion matrix, training curves, sample predictions and Excel report are automatically saved.

---


### ğŸš€ Key Features & Defensive Engineering

This pipeline is engineered for unattended, robust execution in research environments and containerized clusters. It moves beyond simple classification by implementing low-level system safeguards:

**Kernel-Level Singleton** (ensure_single_instance): Utilizes the fcntl.flock Unix syscall to acquire an Exclusive Lock (LOCK_EX | LOCK_NB) on a physical lock-file. This prevents race conditions on GPU VRAM or checkpoint corruption by ensuring only one training instance is active at a time.

**Atomic Run Isolation**: Managed via the RunPaths utility, every execution generates a unique workspace (outputs/YYYYMMDD_HHMMSS/). Logs, high-resolution plots, and Excel reports are isolated to prevent historical data overwrites.

**Proactive Process Guard**: Integrates psutil to identify and terminate ghost Python processes sharing the same entry-point, optimizing resource allocation in shared HPC or Docker environments.

**Data Integrity & Validation**: Implements MD5 Checksum verification for dataset downloads and a strict validate_npz_keys check to ensure the structural integrity of the MedMNIST .npz files before memory allocation.

**Deterministic Pipeline**: Guaranteed bit-per-bit reproducibility (Seed 42) via torch.backends.cudnn.deterministic and custom worker_init_fn to handle RNG seeding across multi-process DataLoaders.

**System Utilities**: The system.py module serves as a low-level abstraction layer that manages hardware device selection, ensures process-level atomicity through kernel-level file locking, and enforces strict environment-wide reproducibility.

**Continuous Stability Guard** (smoke_test.py): A dedicated diagnostic script that executes a "micro-pipeline" (1 epoch, minimal data subset). It validates the entire execution chainâ€”from weight interpolation to Excel reportingâ€”in less than 30 seconds, ensuring no regressions after architectural changes.

---


### ğŸ— Architecture Details: ResNet-18 for 28Ã—28

Standard ResNet-18 is designed for 224Ã—224 inputs. To handle the small-scale MedMNIST manifold without aggressive information loss, the backbone has been modified:

**Stem Adaptation**: The initial 7Ã—7 convolution (stride 2) is replaced with a 3Ã—3 kernel (stride 1).

**Downsampling Removal**: The initial MaxPool layer is bypassed. This maintains a 28Ã—28 feature map depth into the residual blocks, preserving critical morphological details for medical diagnostics.

**Bicubic Weight Transfer**: Pretrained ImageNet weights are preserved; the first layer's weights are adapted to the new 3Ã—3 geometry via bicubic upsampling, maintaining the value of large-scale learned features. To maintain the representational power of the ImageNet-pretrained backbone, weights from the original 7Ã—7 stem are mapped to the adapted 3Ã—3 geometry using bicubic interpolation; this process preserves the spatial distribution of learned feature detectors while aligning them with the new architectural constraints.

| Layer | Standard ResNet-18 | Adapted ResNet-18 (Ours) | Adaptation Strategy |
| :--- | :--- | :--- | :--- |
| **Input Conv** | $7 \times 7$, stride 2, pad 3 | $3 \times 3$, stride 1, pad 1 | Bicubic Weight Interpolation |
| **Max Pooling** | $3 \times 3$, stride 2 | **Disabled (Identity)** | Maintain spatial resolution |
| **Feature Map In** | $56 \times 56$ | $28 \times 28$ | Preserve morphological details |

---

### ğŸ”¬ Mathematical Weight Transfer

To retain the representational power of the pretrained backbone, we do not initialize the new $3 \times 3$ kernel randomly. Instead, we perform a spatial transformation on the weight tensor:

**Source Tensor**: Pretrained ImageNet weights 

$$W_{src} \in \mathbb{R}^{C_{out} \times C_{in} \times 7 \times 7}$$

**Interpolation**: Application of a bicubic resizing function $f$ across spatial dimensions:

$$W_{dest} = f(W_{src}, \text{size}=(3, 3))$$

**Result**: A $3 \times 3$ kernel that preserves the edge-detection patterns learned on ImageNet but optimized for a tighter receptive field.

Note: This process ensures that the model starts training with "meaningful" filters rather than noise, leading to faster convergence and higher accuracy on small-scale medical textures.

---

### ğŸ”¬ Training Regularization

To improve generalization on the $28 \times 28$ manifold, the pipeline implements MixUp during training. New samples are synthesized as a convex combination of two random samples from the mini-batch:

$$
\tilde{x} = \lambda x_i + (1 - \lambda) x_j
$$
$$
\tilde{y} = \lambda y_i + (1 - \lambda) y_j
$$

Where $\lambda \in [0, 1]$ is drawn from a $\text{Beta}(\alpha, \alpha)$ distribution.

---

### ğŸ“ Project Structure

```bash
med_mnist/
â”œâ”€â”€ main.py                   # Global entry point
â”œâ”€â”€ smoke_test.py             # Rapid diagnostic tool (End-to-End check)
â”œâ”€â”€ Dockerfile                # Image definition
â”œâ”€â”€ .dockerignore             # Build optimization
â”œâ”€â”€ .gitignore                # Repository filtering
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ media/                # Stable README assets (tracked)
â”œâ”€â”€ tools/                    # Maintenance (RAM checks, etc.)
â”œâ”€â”€ legacy/                   # Archived code
â”œâ”€â”€ scripts/                  # Modular package
â”‚   â”œâ”€â”€ core/                 # Config & Constants
â”‚   â”œâ”€â”€ data_handler/         # Loading & Augmentation
â”‚   â”œâ”€â”€ models/               # Model Factory
â”‚   â”œâ”€â”€ trainer/              # Training Loop & MixUp
â”‚   â””â”€â”€ evaluation/           # Engine & Reporting
â””â”€â”€ outputs/                  # Results (ignored by Git)
    â””â”€â”€ YYYYMMDD_HHMMSS/      # Timestamped run folder
```

### âš™ï¸ Requirements & Installation

Install dependencies easily with pip:

```bash
pip install -r requirements.txt
```

Install dependencies easily with pip, or check the full list here:

[ğŸ“¦ See Full Requirements](requirements.txt)


### ğŸ’» Usage (Local)

Run the script from the project root. It will default to the fast mode (`num_workers=4`).

```bash
git clone https://github.com/tomrussobuilds/medmnist.git
cd medmnist
python main.py
```
Note: The entry point script is now `main.py`.

The script will automatically:

- Download BloodMNIST if missing
- Train for max 60 epochs with early stopping (`patience=15`)
- Save the best model â†’ `outputs/YYYYMMDD_HHMMSS_bloodmnist_resnet18/models/best_model.pth`
- Generate figures, confusion matrix, Excel report â†’ `figures/` and `reports/`

---

### âœ… Environment Verification (Smoke Test)
Before starting a full training session, it is highly recommended to run the diagnostic smoke test. This ensures that your local environment, PyTorch versions, and visualization libraries are fully compatible:

```bash
python smoke_test.py
```

This will run a 1-epoch training on a tiny subset of a MedMNIST dataset and verify the generation of all output files.

---

### ğŸ³ Docker Execution (Recommended for Portability)

The pipeline is containerized using the included `Dockerfile`.

| Mode | Command | `num_workers` | Guarantees |
| :--- | :--- | :---: | :--- |
| **Fast Mode (Default)** | `docker run bloodmnist_image` | 4 | Best performance on CPU, **Not fully deterministic** |
| **Strict Reproducibility** | `-e DOCKER_REPRODUCIBILITY_MODE=TRUE` | 0 | **100% Deterministic** (bit-per-bit), but slower |

---

Build the image locally using the provided Dockerfile. This ensures all dependencies and environment are correctly configured.

```bash
sudo docker build -t bloodmnist_image .
```

Run in Strict Reproducibility Mode (Recommended for testing):

```bash
sudo docker run -it --rm \
  -e DOCKER_REPRODUCIBILITY_MODE=TRUE \
  -v $(pwd)/dataset:/app/dataset \
  bloodmnist_image
```

### ğŸ“Š Command Line Arguments

You can fully configure training from the command line (via `main.py`).

| Arg | Type | Default | Description |
| :--- | :--- | :--- | :--- |
| --epochs | int | 60 | Maximum number of training epochs. |
| --batch_size | int | 128 | Batch size for data loaders. |
| --lr | float | 0.008 | Initial learning rate for the SGD optimizer. |
| --seed | int | 42 | Random seed for reproducibility (influences PyTorch, NumPy, Python). |
| --mixup_alpha | float | 0.002 | $\alpha$ parameter for MixUp regularization. Set to 0 to disable MixUp. |
| --patience | int | 15 | Early stopping patience (epochs without validation improvement). |
| --momentum | float | 0.9 | Momentum factor for the SGD optimizer. |
| --weight_decay | float | 5e-4 | Weight decay (L2 penalty) for the optimizer |
| --no_tta | flag | (disabled) | Flag to disable Test-Time Augmentation (TTA) during final evaluation. |
| --hflip | float | 0.5 | Probability of Horizontal Flip augmentation. |
| --rotation_angle | int | 10 | Max rotation angle for random rotations. |
| --jitter_val | float | 0.2 | Strength of Color Jitter (brightness/contrast). |
| --dataset | str | "bloodmnist" | MedMNIST dataset identifier (e.g., bloodmnist, dermamnist). |
| --model_name | str | "ResNet-18 Adapted" | Identifier for logging and folder naming. |


Examples:

Run without TTA

```bash
python main.py --no_tta
```
Train for 100 epochs

```bash
python main.py --epochs 100
```
Lower LR for finer tuning

```bash
python main.py --lr 0.001
```
Disable MixUp

```bash
python main.py --mixup_alpha 0
```
Custom batch size & seed

```bash
python main.py --batch_size 64 --seed 123
```
### Scaling to other MedMNIST datasets
Thanks to the new registry system, you can train on different datasets without changing the code:

```bash
python main.py --dataset dermamnist --lr 0.005 --epochs 100
```

### Citation

If you use this repository in academic work or derivative projects:

```bibtex
@misc{medmnist_resnet18,
  title  = {MedMNIST Classification with Adapted ResNet-18},
  author = {Tommaso Russo},
  year   = {2025},
  url    = {https://github.com/tomrussobuilds/medmnist}
}
```
### ğŸ—º Research Goals & Roadmap

This project serves as a sandbox for medical imaging experimentation. While the Adapted ResNet-18 has demonstrated that classic architecturesâ€”when paired with modern training techniques like MixUp and Stem-Adaptationâ€”remain extremely competitive for MedMNIST, it is only the starting point.

  - Phase 1 (Completed): Optimization on 28Ã—28 BloodMNIST. Implementation of kernel stem resizing and MaxPool removal to preserve spatial resolution.

  - Phase 2 (Near Term): Scaling the pipeline across the entire MedMNIST v2 suite (e.g., DermaMNIST, OrganMNIST) via the DATASET_REGISTRY to validate hyperparameter robustness.

  - Phase 3 (Mid Term): Moving beyond ResNet to evaluate how modern backbones like EfficientNet-V2 and ConvNeXt handle the small-scale medical manifold.

  - Phase 4 (Long Term): Transitioning the repo into a versatile benchmarking framework, extending support for high-resolution formats (224Ã—224) and multi-modal integration.
