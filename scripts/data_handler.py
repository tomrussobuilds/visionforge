"""
Data Preparation Module for BloodMNIST

Handles the downloading, loading, and preprocessing of the BloodMNIST dataset.
This includes utilities for robust data fetching, data structuring, defining
a PyTorch Dataset, and creating DataLoaders with appropriate augmentation
(strong augmentation for training, standard normalization for validation/testing).
"""
# =========================================================================== #
#                                Standard Imports
# =========================================================================== #
from pathlib import Path
import time
from typing import Tuple, Final
import requests
from dataclasses import dataclass
import logging
import random

# =========================================================================== #
#                                Third-Party Imports
# =========================================================================== #
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms

# =========================================================================== #
#                                Internal Imports
# =========================================================================== #
from scripts.core import (
    Config, Logger,
    md5_checksum, validate_npz_keys,
    BLOODMNIST_CLASSES, EXPECTED_MD5, URL, NPZ_PATH, FIGURES_DIR
    )

# Global logger instance
logger: Final[logging.Logger] = Logger().get_logger()

# =========================================================================== #
#                                 Data Utilities
# =========================================================================== #

def worker_init_fn(worker_id: int):
    """
    Initializes random number generators (PRNGs) for each DataLoader worker.
    This ensures data transformations are deterministic per worker process.

    Args:
        worker_id (int): ID of the worker process.
    """
    # Use the base configuration seed
    initial_seed = Config().seed
    
    # Create a unique seed for the worker
    worker_seed = initial_seed + worker_id 
    
    # Apply the seed to standard PRNGs used by transformations
    np.random.seed(worker_seed)
    random.seed(worker_seed) 
    torch.manual_seed(worker_seed)
    
    # PyTorch handles its own PRNG for the DataLoader internally 
    # based on this logic, so no need for an explicit torch.manual_seed.
    pass

def ensure_mnist_npz(
        target_npz: Path,
        retries: int = 5,
        delay: float = 5.0,
        cfg: Config | None = None
    ) -> Path:
    """
    Downloads the BloodMNIST dataset NPZ file robustly, with retries and MD5 validation.

    Args:
        target_npz (Path): The expected path for the dataset NPZ file.
        retries (int): Max number of download attempts.
        delay (float): Delay (seconds) between retries.

    Returns:
        Path: Path to the successfully validated .npz file.

    Raises:
        RuntimeError: If the dataset cannot be downloaded and verified.
    """
    def _is_valid(path: Path) -> bool:
        """Checks file existence, size, and MD5 checksum."""
        if not path.exists() or path.stat().st_size < 50_000:
            return False
        # Check for ZIP header (NPZ files are ZIP archives)
        if path.read_bytes()[:2] != b"PK":
            return False
        return md5_checksum(path) == EXPECTED_MD5

    if _is_valid(target_npz):
        logger.info(f"Valid dataset found: {target_npz}")
        return target_npz

    if target_npz.exists():
        logger.warning(f"Corrupted or incomplete dataset found, deleting: {target_npz}")
        target_npz.unlink()

    logger.info(f"Downloading {cfg.model_name if cfg else 'dataset'} from {URL}")
    tmp_path = target_npz.with_suffix(".tmp")
    
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(URL, timeout=60)
            response.raise_for_status() 
            tmp_path.write_bytes(response.content)

            if not _is_valid(tmp_path):
                raise ValueError("Downloaded file failed validation (wrong size/header/MD5)")

            tmp_path.replace(target_npz) # Atomic move
            logger.info(f"Successfully downloaded and verified: {target_npz}")
            return target_npz

        except Exception as e:
            if tmp_path.exists():
                tmp_path.unlink()

            if attempt == retries:
                model_info = cfg.model_name if cfg else "dataset"
                logger.error(f"Failed to download dataset after {retries} attempts")
                raise RuntimeError(f"Could not download {model_info} dataset") from e

            logger.warning(f"Attempt {attempt}/{retries} failed: {e}. Retrying in {delay}s...")
            time.sleep(delay)

    raise RuntimeError("Unexpected error in dataset download logic.")


@dataclass(frozen=True)
class BloodMNISTData:
    """A container for the BloodMNIST dataset splits stored as NumPy arrays."""
    X_train: np.ndarray
    y_train: np.ndarray
    X_val: np.ndarray
    y_val: np.ndarray
    X_test: np.ndarray
    y_test: np.ndarray


def load_bloodmnist(npz_path: Path = NPZ_PATH,
                    cfg: Config | None = None
) -> BloodMNISTData:
    """
    Loads the dataset from the NPZ file, validates its keys, and returns
    the structured data splits.
    
    Args:
        npz_path (Path, optional): Path to the NPZ file.

    Returns:
        BloodMNISTData: The structured dataset splits.
    """
    path = ensure_mnist_npz(npz_path, cfg=cfg)

    logger.info(f"Loading dataset from {path}")

    # Use mmap_mode="r" for memory efficiency
    with np.load(npz_path, mmap_mode="r") as data:
        validate_npz_keys(data)
        logger.info(f"Keys in NPZ file: {data.files}")

        return BloodMNISTData(
            X_train=data["train_images"],
            X_val=data["val_images"],
            X_test=data["test_images"],
            y_train=data["train_labels"].ravel(), # Flatten labels
            y_val=data["val_labels"].ravel(),
            y_test=data["test_labels"].ravel(),
        )


class BloodMNISTDataset(Dataset[Tuple[torch.Tensor, torch.Tensor]]):
    """
    PyTorch Dataset for BloodMNIST data. Handles NumPy to Tensor conversion
    and optional image transformations.
    """
    def __init__(self,
                 images: np.ndarray,
                 labels: np.ndarray,
                 transform: transforms.Compose | None = None):
        """
        Args:
            images (np.ndarray): Image data (H, W, C).
            labels (np.ndarray): Label data.
            transform (transforms.Compose | None): Torchvision transformations.
        """
        # Normalize pixel values to [0.0, 1.0]
        self.images = images.astype(np.float32) / 255.0
        # Convert labels to int64 (long) for loss function
        self.labels = labels.astype(np.int64) 
        self.transform = transform

    def __len__(self) -> int:
        return len(self.labels)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        img = self.images[idx]
        label = self.labels[idx]

        if self.transform:
            # Apply transformation (e.g., augmentation for training)
            img = self.transform(img)
        else:
             # Convert NumPy array (H, W, C) to PyTorch Tensor (C, H, W)
             img = torch.from_numpy(img).permute(2, 0, 1)


        return img, torch.tensor(label, dtype=torch.long)

def get_augmentations_transforms(cfg: Config) -> str:
    """
    Generates a descriptive string of the augmentations using values from Config.
    """ 
    return (
        f"RandomHorizontalFlip({cfg.hflip}), "
        f"RandomRotation({cfg.rotation_angle}), "
        f"ColorJitter ({cfg.jitter_val}), "
        f"RandomResizedCrop(28, scale=(0.9, 1.0)), "
        f"MixUp(alpha={cfg.mixup_alpha})"
    )

def get_dataloaders(
        data: BloodMNISTData,
        cfg: Config,
        ) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    Creates PyTorch DataLoaders for train, validation, and test splits.

    Uses strong augmentation for training and standard normalization for validation/testing.

    Args:
        data (BloodMNISTData): The structured dataset splits.
        cfg (Config): Configuration containing batch_size and num_workers.

    Returns:
        Tuple[DataLoader, DataLoader, DataLoader]: (train_loader, val_loader, test_loader).
    """
    IMG_SIZE: Final[int] = 28
    NORM_MEAN: Final[Tuple[float, float, float]] = (0.485, 0.456, 0.406)
    NORM_STD: Final[Tuple[float, float, float]] = (0.229, 0.224, 0.225)

    # 1. Define Augmentations and Transformations
    # Strong augmentation for the training set
    train_transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.RandomHorizontalFlip(p=cfg.hflip),
        transforms.RandomRotation(cfg.rotation_angle),
        transforms.ColorJitter(
            brightness=cfg.jitter_val,
            contrast=cfg.jitter_val,
            saturation=cfg.jitter_val
        ),
        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0)),
        transforms.ToTensor(), # Scales to [0, 1] and converts to (C, H, W)
        # ImageNet normalization
        transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
    ])
    
    # Standard normalization for validation and test sets
    val_transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.ToTensor(),
        transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
    ])

    # 2. Create Datasets
    train_ds = BloodMNISTDataset(data.X_train, data.y_train, transform=train_transform)
    val_ds   = BloodMNISTDataset(data.X_val,   data.y_val,   transform=val_transform)
    test_ds  = BloodMNISTDataset(data.X_test,  data.y_test,  transform=val_transform)

    # Apply worker_init_fn only if parallel workers are used
    init_fn = worker_init_fn if cfg.num_workers > 0 else None
    
    # 3. Create DataLoaders
    pin_memory = torch.cuda.is_available() # Pin memory if CUDA is available

    train_loader = DataLoader(
        train_ds,
        batch_size=cfg.batch_size,
        shuffle=True,
        num_workers=cfg.num_workers,
        pin_memory=pin_memory,
        worker_init_fn=init_fn
    )
    val_loader   = DataLoader(
        val_ds,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        pin_memory=pin_memory,
        worker_init_fn=init_fn
    )
    test_loader  = DataLoader(
        test_ds,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        pin_memory=pin_memory,
        worker_init_fn=init_fn
    )
    
    # Log dataset sizes for confirmation
    logger.info(f"Dataset loaded → Train:{len(train_ds)} | Val:{len(val_ds)} | Test:{len(test_ds)}")
    
    return train_loader, val_loader, test_loader


# =========================================================================== #
#                               SAMPLE IMAGES
# =========================================================================== #

def show_sample_images(
        data: BloodMNISTData,
        save_path: Path | None = None,
        cfg: Config | None = None
    ) -> None:  
    """
    Generates and saves a figure showing 9 random samples from the training set.

    Args:
        data (BloodMNISTData): The structured dataset (to access training images).
        save_path (Path | None, optional): Path to save the figure.
                                           Defaults to FIGURES_DIR/bloodmnist_samples.png.
    """
    if save_path is None:
        save_path = FIGURES_DIR / f"{cfg.model_name}_samples.png"

    if save_path.exists():
        logger.info(f"Sample images figure already exists → {save_path}")
        return

    indices = np.random.choice(len(data.X_train), size=9, replace=False)

    plt.figure(figsize=(9, 9))
    for i, idx in enumerate(indices):
        img = data.X_train[idx]
        label = int(data.y_train[idx])

        plt.subplot(3, 3, i + 1)

        # Handle grayscale (1 channel) or color (3 channels) images
        if img.ndim == 3 and img.shape[-1] == 3:
            plt.imshow(img)
        else:
            plt.imshow(img.squeeze(), cmap='gray')

        plt.title(f"{label} — {BLOODMNIST_CLASSES[label]}", fontsize=11)
        plt.axis("off")

    plt.suptitle(f"{cfg.model_name} — 9 Random Samples from Training Set", fontsize=16)
    # Adjust layout to prevent title overlap
    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) 
    plt.savefig(save_path, dpi=200, bbox_inches="tight")
    plt.close()
    logger.info(f"Sample images saved → {save_path}")