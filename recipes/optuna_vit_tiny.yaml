# ==============================================================================
# FILE: recipes/optuna_vit_tiny.yaml
# Hyperparameter Optimization for Vision Transformer Tiny (224x224)
# ==============================================================================
# Usage: python forge.py --config recipes/optuna_vit_tiny.yaml
# Estimated Time: ~3-5 hours (20 trials × 15 epochs)
# Memory Usage: ~4GB VRAM per trial with AMP
#
# OPTUNA SEARCH SPACE (orchard/optimization/search_spaces.py):
#   OPTIMIZED PARAMETERS (ranges):
#     - Learning rate: 1e-5 to 1e-2 (log scale)
#     - Weight decay: 1e-6 to 1e-3 (log scale)
#     - Momentum: 0.85 to 0.95
#     - Batch size: [16, 32, 48, 64] (28px) | [8, 12, 16] (224px)
#     - Dropout: 0.1 to 0.5
#     - MixUp alpha: 0.0 to 0.4
#     - Label smoothing: 0.0 to 0.2
#     - Cosine fraction: 0.3 to 0.7
#     - Scheduler patience: 3 to 10
#     - Augmentation: rotation (0-15°), jitter (0-0.15), scale (0.9-1.0)
#     - Model architecture (if enable_model_search: true):
#         - 28px: mini_cnn vs resnet_18
#         - 224px: efficientnet_b0 vs vit_tiny (+ ViT weight variants)
#
#   FIXED PARAMETERS (not optimized):
#     - epochs, patience, grad_clip, scheduler_type
#     - use_amp, use_tta
#     - criterion_type (options: "cross_entropy" | "focal")
#     - focal_gamma (only used if criterion_type: "focal")
#     - weighted_loss

dataset:
  name: "dermamnist"
  data_root: ./dataset
  resolution: 224
  force_rgb: true
  use_weighted_sampler: true
  max_samples: null

architecture:
  name: "vit_tiny"
  pretrained: true
  weight_variant: "vit_tiny_patch16_224.augreg_in21k_ft_in1k"
  dropout: 0.1                  # Will be optimized

training:
  seed: 42
  
  # Base values (WILL BE OPTIMIZED BY OPTUNA)
  batch_size: 16                # Conservative for optimization
  learning_rate: 0.0003         # ViT baseline
  weight_decay: 0.0001
  momentum: 0.9
  min_lr: 1e-7
  
  # Regularization (WILL BE OPTIMIZED)
  mixup_alpha: 0.0
  label_smoothing: 0.0
  
  # Training loop
  epochs: 60                    # Final training only
  patience: 12
  grad_clip: 1.0
  mixup_epochs: 0
  
  # Scheduler (PARTIALLY OPTIMIZED)
  scheduler_type: "cosine"
  cosine_fraction: 0.5
  scheduler_patience: 5
  scheduler_factor: 0.1
  step_size: 20
  
  # Performance (CRITICAL FOR MEMORY)
  use_amp: true                 # MUST be true for 224x224
  use_tta: false
  criterion_type: "cross_entropy"
  weighted_loss: false
  focal_gamma: 2.0              # Only used if criterion_type: "focal"

augmentation:
  hflip: 0.5
  rotation_angle: 5             # Will be optimized
  jitter_val: 0.1               # Will be optimized
  min_scale: 0.97               # Will be optimized
  tta_translate: 0.5
  tta_scale: 1.02
  tta_blur_sigma: 0.1

hardware:
  device: "auto"
  reproducible: true

telemetry:
  output_dir: ./outputs
  log_level: "INFO"
  log_interval: 50

evaluation:
  batch_size: 32
  n_samples: 12
  fig_dpi: 200
  cmap_confusion: Blues
  plot_style: seaborn-v0_8-muted
  grid_cols: 4
  fig_size_predictions: [12, 8]
  report_format: xlsx
  save_confusion_matrix: false  # Skip during optimization
  save_predictions_grid: false

optuna:
  study_name: "dermamnist_vit_tiny_optimized"
  n_trials: 20                  # Balanced exploration
  epochs: 15                    # Shorter for speed
  timeout: null
  
  # Optimization target
  metric_name: "auc"
  direction: "maximize"
  
  # Early stopping config
  enable_early_stopping: true
  early_stopping_threshold: 0.9999
  early_stopping_patience: 2
  
  # Search strategy
  sampler_type: "tpe"
  search_space_preset: "full"
  enable_model_search: true     # Compare efficientnet_b0 vs vit_tiny + weight variants

  # Pruning
  enable_pruning: true
  pruner_type: "median"
  pruning_warmup_epochs: 5
  
  # Storage
  storage_type: "sqlite"
  storage_path: null
  
  # Execution
  n_jobs: 1
  load_if_exists: true
  show_progress_bar: false
  
  # Output
  save_plots: true
  save_best_config: true

export:
  format: onnx
  opset_version: 18
  validate_export: true
